---
title: "Tuning minimal generalisations on a morphological learning task"
author: "Rácz, Péter"
date: "`r format(Sys.Date(),'%e %B, %Y')`"
output: github_document
---

```{r setup, include=FALSE}

# -- head -- #

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, fig.path = 'figures/', fig.width = 1, fig.height = 1)
knitr::opts_knit$set(root.dir = '~/Github/Racz2024rules/')

setwd('~/Github/Racz2024rules/')

library(tidyverse)
library(magrittr)
library(glue)
library(knitr)
library(kableExtra)
library(ggthemes)
library(patchwork)
library(lme4)
library(broom.mixed)
library(performance)
library(sjPlot)
library(scales)
library(googlesheets4)
library(pROC)

# -- auth -- #

gs4_auth(email = 'petermartonracz@gmail.com')

# -- read -- #

# esp data with baseline word log odds and rules, made by scripts/compile_data.R
d = read_tsv('dat/exp_data_with_rules.gz')
# modelling outcomes, with best parameter settings of learning rate, made by scripts/learner.R
outcomes = read_tsv('dat/modelling_best_outcomes.tsv')
# best modelling outcome according to correlation of word weight and word mean resp, made by scripts/learner.R
# accuracy curves across learning rate
curves = read_tsv('dat/modelling_curve.tsv')
test2_2 = read_tsv('dat/posttest_data_original_with_best_rules.gz') # test2 with updating mgl preds
# -- wrangle -- #

# sort out factor levels
d %<>%
  mutate(
    phase = str_replace_all(phase, c('test1' = 'pretest', 'test2' = 'posttest')),
    phase = factor(phase, levels = c('pretest', 'esp', 'posttest')),
    reg_rate = factor(reg_rate, levels = c('-40%', 'nc', '+40%')),
    reg_dist = str_replace(lex_typicality, 'atypical', 'reversed'),
    reg_dist = factor(reg_dist, levels = c('reversed', 'random', 'typical')),
    type = factor(type, levels = c('regular', 'irregular'))
  )

```

A cognitively plausible model of people's morphophonological intuitions in a Wug task is the Minimal Generalisation Learner (MGL). The MGL looks for generalisations of the type `CAD ~ CBD`, which can be expressed as a rule `A -> B` with the structural description of `C _ D`. We can train the MGL on existing words and it will predict people's responses to nonword stimuli. Here, we explore how the MGL can include short-term learning based on nonword stimuli. We use the data collected by Rácz, Beckner, Hay & Pierrehumbert (2020), who ran a baseline Wug task and a morphological learning task (which they call the ESP task), using an artificial coplayer. Both tasks focussed on the regular / irregular variation of the English past tense.

Table 1 shows an example of how the learner works.

```{r learnerexample}
t1 = d |> 
  filter(word == 'splink', best_rule_word_type) |> 
  distinct(rule_tidy,reliability,scope,hits,confidence,related_forms,exceptions) |> 
  mutate_if(is.double, ~ round(., 2)) |> 
  mutate(rule_number = 1:2) |> 
  relocate(rule_number, 1)

# t1 |> 
#   write_sheet('https://docs.google.com/spreadsheets/d/1RE2MWEZhyh4cwsOHek_T8VkAQ3O5aiowtTeE_meL1lU/edit?usp=sharing', 'table1')
t1 |> 
  kable(digits = 2, caption = '1. Example rules for "splink"')
```

# The baseline experiment

## Nonwords

Rácz, Beckner, Hay & Pierrehumbert (2020) generated nonword verbs across four regular/irregular categories:

- drove ([aI]/[i] → [oU])
- sang ([I] → [ae])
- kept ([i] → [E]Ct)
- burnt ([3]/[E]/[I] → [3]/[E]/[I]Ct)

Nonwords were transcribed into the DISC phonetic alphabet. Examples are in Table 2.

```{r examples}
t2 = d |>
  mutate(word2 = glue('{word}, [{disc}]')) |> 
  group_by(category) |> 
  distinct(word2,category) |> 
  sample_n(5) |> 
  mutate(id = 1:5) |> 
  pivot_wider(names_from = category, values_from = word2) |> 
  select(-id) 

t2 |> 
  write_sheet('https://docs.google.com/spreadsheets/d/1RE2MWEZhyh4cwsOHek_T8VkAQ3O5aiowtTeE_meL1lU/edit?usp=sharing', 'table2')
t2 |> 
  kable('simple', caption = '2. Nonword examples.')

```

## Test data

202 participants, recruited on AMT, responded to the orthographic present tense form of each nonword in a simple carrier sentence in a forced-choice task. They could pick the regular or the irregular past tense form for each nonword, displayed on buttons. The regular past tense form was the -ed form. The irregular form depended on the verb class, as seen in Table 3.

```{r examples2}
t3 = d |> 
  distinct(category,word,regular_form,irregular_form) |> 
  group_by(category) |> 
  sample_n(2)

t3 |> 
  write_sheet('https://docs.google.com/spreadsheets/d/1RE2MWEZhyh4cwsOHek_T8VkAQ3O5aiowtTeE_meL1lU/edit?usp=sharing', 'table3')
t3 |> 
  kable('simple', caption = '3. Regular and irregular choices in the Wug task.')
```

## Minimal Generalisation Learner (MGL)

Rácz, Beckner, Hay and Pierrehumbert (2020) trained the Minimal Generalisation Learner (MGL) on English verbs in CELEX and used it to make predictions for the nonwords. They trained the MGL on regular and irregular English verbs with a minimum frequency cutoff of 10: 4160 past/present verb transcriptions. They used the best parameters identified by Albright & Hayes (2003) for a similar task: lower and upper confidence limits of 55% and 95%.

The MGL generates `r length(unique(d$rule_id))` rules for the 156 target forms from the training data. Such rules have a structural description that matches a target nonword in the task and generates an output which is available to participants to pick. A rule that generates the `sing -> sang` pattern matches target forms for nonwords that look like `sing`. It generates one of the past tense forms available in the forced-choice task. A rule that generates the `sing -> sung` pattern does not generate an available past tense form.

```{r rules1}
t4 = d |> 
  filter(
    category == 'sang',
    !is.na(rule),
    # best_rule_word_type
         ) |> 
  mutate(
    rule_tidy = str_replace_all(rule_tidy, c('1 ' = '+', '0 ' = '-'))
  ) |> 
  distinct(rule_tidy,type,scope,hits,reliability,confidence,related_forms,exceptions) |> 
  arrange(-confidence) |> 
  mutate_if(is.double, ~ round(.,2))

t4 |> 
  write_sheet('https://docs.google.com/spreadsheets/d/1RE2MWEZhyh4cwsOHek_T8VkAQ3O5aiowtTeE_meL1lU/edit?usp=sharing', 'table4')
t4 |> 
  kable('simple', caption = '4. Rules from Celex.', digits = 2)
```

Rules take the structural description of input -> output / context. Multiple rules can apply to the same input form. For the majority of forms, there is one regular and one irregular rule available. For some, there is no irregular rule. For some, there are more regular rules. Let's look at rules for the "sang" group only. Rules can be seen in Table 4. Examples can be seen in Table 5.

```{r rules2}
t5 = d |> 
  filter(
    word %in% c('shing', 'pring', 'grink'),
    best_rule_word_type
         ) |> 
  distinct(word,category,type,rule_tidy,confidence,weight) |> 
  mutate_if(is.double, ~ round(.,2))
  
t5 |> 
  write_sheet('https://docs.google.com/spreadsheets/d/1RE2MWEZhyh4cwsOHek_T8VkAQ3O5aiowtTeE_meL1lU/edit?usp=sharing', 'table5')
t5 |> 
  kable('simple', caption = 'Table 5. Best regular / irregular rules for some sang forms.', digits = 2)

```

Following both Rácz, Beckner, Hay & Pierrehumbert (2020) and Albright & Hayes (2003) we can pick the best regular and the best irregular rule and calculate a weight, which is the confidence of the best regular rule / (the confidence of the best regular rule + the confidence of the best irregular rule). If there is no irregular rule, this will default to 1. We will work with the rules that are best rules for any form and call these the relevant rules.

## Results

```{r baseline_156, fig.width = 7, fig.height = 3}
cor = d |> 
  distinct(word,baseline_log_odds_regular,weight) |> 
  summarise(
    cor = cor(baseline_log_odds_regular,weight)
  ) |> 
  pull(cor) |> 
  round(2)

p1 = d |> 
  distinct(word,baseline_log_odds_regular,weight) |> 
  ggplot(aes(weight,baseline_log_odds_regular)) +
  geom_point(alpha = .5) +
  geom_smooth(method = 'loess') +
  theme_bw() +
  xlab('learner regular weight') +
  ylab('log (regular/irregular), baseline task') +
  ggtitle(glue('Words: Word ratings and regular weights (r = {cor}).'))

p2 = d |> 
  filter(best_rule_word_type) |> 
  distinct(word,baseline_log_odds_regular,type,confidence) |> 
  group_by(type) |> 
  mutate(scaled_confidence = rescale(confidence)) |> 
  ggplot(aes(scaled_confidence,baseline_log_odds_regular,colour = type)) +
  geom_point(alpha = .5) +
  geom_smooth(method = 'loess') +
  theme_bw() +
  theme(axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank()) +
  scale_colour_viridis_d(option = 'cividis') +
  xlab('learner rule confidence') +
  labs(colour = 'rule type') +
  ggtitle('Words: Word ratings and regular / irregular rule confidence (rescaled)')

p1 + p2 + plot_annotation(title = 'Figure 1. Baseline task: Word ratings and MGL predictions')
```

Figure 1 shows how MGL predictions correlate with participant responses in the baseline etask. The left panel shows the relationship between word weights (x axis) and the log odds of regular and irregular choices made by participants in the baseline task (y axis). The right panel breaks down the weight into its two components: the best regular and irregular rule for each word. Since, on the whole, regular rules have higher confidence than irregular rules, we rescaled rule confidence across these groups so they are more directly comparable. 

The MGL weights correlate with the response odds. We see that the trajectory of this relationship is built up from two opposite trajectories for best rules. For words with low regular weight, this weight comes from irregular rules that have low confidence themselves but, relatively speaking, outweigh the relevant regular rules. For words with higher regular weight, this comes from two things. First, high-confidence, large-scale regular rules apply to these, and these rules bring up the regular weight. Second, the relevant irregular rules have very low confidence. This breakdown of the MGL's regular weight will become relevant later.

Note that, from the MGL's perspective, the main technical difference between regular and irregular rules is that regular rules tend to have broader structural descriptions and will fit more existing words. Whether a rule generates an `-ed` ending or e.g. changes a vowel is not structurally different from the model's point of view.

Rácz, Beckner, Hay & Pierrehumbert (2020) and Albright & Hayes (2003) likewise find that the minimal generalisations (rules) of the MGL are more accurate in predicting participant responses than an instance-based learner, despite the higher level of abstraction.

# The ESP experiment

Rácz, Beckner, Hay & Pierrehumbert (2020) ran a second online experiment using new participants and the nonwords from the baseline experiment. Each participant went through three blocks. First, in the pretest phase, they responded to 52 standalone nonwords in a forced-choice task, identical to the baseline experiment. Second, in the ESP test phase, they responded to a new set of 52 nonwords. This time they were playing against a co-player and had to guess the coplayer's pick in each trial. Correct guesses were rewarded with a point. Coplayer behaviour was based on the participant's specific pretest behaviour and the baseline data. Third, in the posttest phase, they responded to a new set of 52 nonwords, playing alone again. The ESP design is used widely in tasks where it is important for descriptions to match, like image tagging. ESP refers to the fact that participans have to "read" each other's minds to converge on a description. In this particular case, the participant had to do all the mindreading, since the coplayer's choices were set in advance.

Coplayers varied across two conditions. In terms of (A) rate of regularisation, the coplayer had (i) the same regularisation rate as the participant in the pretest, (ii) regularised 40% more verbs, (iii) regularised 40% fewer verbs. Participants who regularised too much or too little (so that the entire effect of this shift would have been capped by the floor or the ceiling of the 52 verbs in the ESP test) were excluded. In terms of (B) lexical distribution, the coplayer regularised the first n% verbs (n depending on A) that were rated most regular in the baseline task (the typical coplayer), the first n% verbs that were rated most irregular in the baseline task (the reversed coplayer), or n% verbs at random (the random coplayer). This means that a typical coplayer makes choices that are characteristic of an average participant. A reversed coplayer turns these choices upside down.

## Results

Rácz, Beckner, Hay & Pierrehumbert (2020) found that the coplayer changed participant behaviour. Nonword ratings shifted from the pretest to the posttest. If participants rated words as highly regular in the pretest, they rated these more irregular in the posttest, after interacting with the reversed versus the typical coplayer. Since no participant saw the same verb twice, this effect was due to lexical, rather than word priming. The reversed coplayer used verbs in a certain way, and the participant rated similar verbs in the posttest in a certain way. The difference with the random coplayer was less clearcut. This can be seen in Figure 2.

```{r mainres1, fig.width = 4, fig.height = 3}
d |>
  filter(phase != 'esp') |> 
  count(word,reg_dist,phase,resp_reg) |> 
  pivot_wider(names_from = resp_reg, values_from = n) |> 
  mutate(log_odds = log(`1`/`0`)) |>
  select(word,reg_dist,phase,log_odds) |> 
  pivot_wider(names_from = phase, values_from = log_odds) |> 
  ggplot(aes(pretest,posttest,colour = reg_dist)) +
  geom_point(alpha = .5) +
  geom_smooth(method = 'lm') +
  theme_bw() +
  scale_colour_viridis_d(option = 'viridis') +
  xlab('log (regular/irregular), pretest') +
  ylab('log (regular/irregular), posttest') +
  labs(colour = 'coplayer\ndistribution') +
  ggtitle('Figure 2. ESP task: Word ratings in\npretest and posttest')
  
```

Figure 2 compares response log odds in the pretest and the posttest. These are correlated: participants make similar choices at the beginning of the experiment and at the end, after encountering the coplayer. However, this correlation is weaker if participants played a reversed coplayer, demonstrating the coplayer's influence.

```{r posttest_models, eval = F}
dwp = dw |> 
  filter(phase == 'posttest')

glmm1 = glmer(resp_reg ~ 1 + reg_dist * reg_rate * weight + (1|participant_id) + (1|word), data = dw, family = binomial, control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))

glmm2 = glmer(resp_reg ~ 1 + reg_dist * weight + reg_rate * weight + (1|participant_id) + (1|word), data = dw, family = binomial, control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))

glmm3 = glmer(resp_reg ~ 1 + reg_dist * weight + reg_rate + (1|participant_id) + (1|word), data = dw, family = binomial, control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))

glmm4 = glmer(resp_reg ~ 1 + reg_dist + reg_rate * weight + (1|participant_id) + (1|word), data = dw, family = binomial, control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))

glmm5 = glmer(resp_reg ~ 1 + reg_dist + reg_rate + weight + (1|participant_id) + (1|word), data = dw, family = binomial, control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))

plot(compare_performance(glmm1,glmm2,glmm3,glmm4,glmm5, metrics = 'common'))
test_likelihoodratio(glmm2,glmm3)
test_likelihoodratio(glmm5,glmm3)

# glmm3 (-^-)
plot_model(glmm, 'pred', terms = c('weight','reg_dist','reg_rate')) +
  theme_bw() +
  scale_fill_viridis_d(option = 'turbo') +
  scale_color_colorblind() +
  labs(colour = 'coplayer\ndistribution') +
  ylab('% response regular') +
  xlab('MGL regular weight') +
  ggtitle('Figure X. The MGL has different accuracy\nacross coplayer lexical distribution.')
```

# What changes in the posttest

The MGL predicts participant responses in general. So, if we look at how MGL prediction accuracy varies across coplayers in the posttest, we find a pattern similar to how these responses shift in the posttest.

The main effect of interest is coplayer lexical distribution. The MGL weight has a stronger effect on posttest responses if the participant played a typical coplayer. This makes sense: a typical coplayer reinforces existing lexical distributions. The MGL's predictive power diminishes when it is set against participants who met a reversed coplayer. What are the mechanics of this shift?

```{r mglbreakdown1, fig.width = 7, fig.height = 12}

p3 = d |>
  filter(phase == 'posttest') |> 
  count(word,reg_dist,weight,resp_reg) |> 
  pivot_wider(names_from = resp_reg, values_from = n) |> 
  mutate(log_odds = log(`1`/`0`)) |>
  ggplot(aes(weight,log_odds,colour = reg_dist)) +
  geom_point(alpha = .5) +
  geom_smooth(method = 'loess') +
  theme_bw() +
  scale_colour_viridis_d(option = 'viridis') +
  xlab('learner regular weight') +
  # ylab('log (regular/irregular), posttest') +
  ylab('') +
  labs(colour = 'coplayer\ndistribution') +
  ggtitle('Words: Word ratings and learner weights')

p4 = d |>
  filter(
    phase == 'posttest',
    best_rule_word_type
         ) |> 
  count(word,reg_dist,rule,rule_id,confidence,type,resp_reg) |> 
  pivot_wider(names_from = resp_reg, values_from = n) |> 
  mutate(log_odds = log(`1`/`0`)) |>
  group_by(type) |> 
  mutate(scaled_confidence = rescale(confidence)) |> 
  ggplot(aes(scaled_confidence,log_odds,colour = type)) +
  geom_point(alpha = .5) +
  geom_smooth(method = 'loess') +
  theme_bw() +
  scale_colour_viridis_d(option = 'cividis') +
  xlab('learner rule confidence') +
  ylab('log (regular/irregular), posttest') +
  labs(colour = 'rule type') +
  facet_wrap( ~ reg_dist) +
  ggtitle('Words: Word ratings and regular / irregular rule confidence (rescaled)')

p5 = d |>
  filter(
    phase == 'posttest',
    best_rule_word_type
         ) |> 
  count(reg_dist,rule,rule_id,confidence,type,resp_reg) |> 
  pivot_wider(names_from = resp_reg, values_from = n) |> 
  mutate(log_odds = log(`1`/`0`)) |>
  group_by(type) |> 
  mutate(scaled_confidence = rescale(confidence)) |> 
  ggplot(aes(scaled_confidence,log_odds,colour = type)) +
  geom_point(alpha = .5) +
  geom_smooth(method = 'loess') +
  theme_bw() +
  scale_colour_viridis_d(option = 'cividis') +
  xlab('learner rule confidence') +
  # ylab('log (regular/irregular), posttest') +
  ylab('') +
  labs(colour = 'rule type') +
  facet_wrap( ~ reg_dist) +
  ggtitle('Rules: Word ratings and regular / irregular rule confidence (rescaled)')

(p3 + plot_spacer()) / p4 / p5 + plot_annotation('Figure 3. ESP posttest: Word ratings in posttest\nand learner weights.')
  
```

Figure 3 shows the correlation of MGL predictions with participant responses in the posttest, split across coplayer lexical distribution.

The top panel shows individual word weights. It is similar to Figure 2. The MGL is the best at predicting the posttest condition that is closest to the baseline, which is the typical condition. The correlation is weaker for responses by participants who encountered a reversed or a random coplayer. The middle panel shows this relationship broken down to the two contributing factors to an MGL weight: the best regular and the best irregular rule for each word. Looking at Figure 2, we said that low weights follow from relevant irregular rules outweighing regular rules in confidence, while, for high weights, this relationship is reversed. Here we see that participants follow this pattern in the typical condition but diverge from it in the reversed and random conditions: their choices reflect a smaller difference between regular and irregular rules than what is predicted by the MGL, and this means that the MGL undershoots irregular verbs and overshoots regular verbs. We see the same relationship in the bottom panel, where we look at confidences and ratings aggregated over individual rules rather than individual words. For each rule, we count the regular and irregular posttest responses for all the nonword verbs in its scope, given that, for each verb, no rule of higher confidence was available. These are the verbs for which this is the best regular / irregular rule. We then plot this against the rule's confidence. Participants in the random and reversed conditions act as if the regular rules and the irregular rules were closer to each other, which is why the MGL has lower accuracy than in the typical condition (or the baseline task).

Posttest patterns come to existence during the interactive session with the coplayer, the ESP task. Participants gradually diverge from the MGL predictions during the ESP task. We visualise this in Figure 4. The top panel shows the relationship between MGL weights and participant responses in each of the 52 trials of the ESP task. For each trial, we calculated a Pearson correlation between word weights and participant responses across the three coplayer lexical distributions. Trials are shown on the x axis. The correlations are shown on the y axis. The correlations vary a lot, so we only plot a loess smooth for each condition. We see that the correlations hold steady for the typical condition, where the coplayer makes lexically typical choices. They gradually deteriorate across the other conditions, in which the coplayer's choices go against the participants' (and the MGL's) expected lexical distributions. The bottom panel breaks this down into regular and irregular rules. The two rule types move in tandem: it is not the case that the overall trajectory shifts because of the behaviour of the regular rules, for instance.

```{r mglbreakdown2, fig.width = 7, fig.height = 6}

trials = d |> 
  filter(phase == 'esp') |> 
  arrange(trial_index) |> 
  group_by(trial_index,reg_dist) |> 
  summarise(
    corw = cor(resp_reg,weight, method = 'pearson'),
    regular = cor(resp_reg,regular, method = 'pearson'),
    irregular0 = cor(resp_reg,irregular, method = 'pearson')
  )

p6 = trials |> 
  ggplot(aes(trial_index,corw,colour = reg_dist)) +
  geom_smooth(method = 'loess') +
  theme_bw() +
  facet_wrap( ~ reg_dist) +
  scale_colour_viridis_d(option = 'viridis') +
  xlab('ESP trial') +
  ylab('correlation of\nlearner regular weight and\nregular response') +
  labs(colour = 'coplayer\ndistribution') +
  ggtitle('ESP trajectories for learner weight\ncorrelation with responses')

p7 = trials |> 
  mutate(irregular = abs(irregular0)) |> 
  select(trial_index,reg_dist,regular,irregular) |> 
  pivot_longer(c(regular,irregular), names_to = 'type') |> 
  ggplot(aes(trial_index,value,colour = type)) +
  geom_smooth(method = 'loess') +
  theme_bw() +
  scale_colour_viridis_d(option = 'cividis') +
  facet_wrap( ~ reg_dist) +
  xlab('ESP trial') +
  ylab('correlation of\nlearner rule confidence and\nregular response') +
  labs(colour = 'rule type') +
  ggtitle('ESP trajectories for learner regular/irregular confidence\ncorrelation with responses')

p6 / p7 + plot_annotation(title = 'Figure 4. Rule weight and confidence across ESP.')

```

# Modelling

The main result of Rácz, Beckner, Hay & Pierrehumbert (2020) is that, when you expose participants to a lexical distribution in the ESP task, they will extend this distribution to previously unseen forms in the posttest, to some extent.

The MGL can capture this shift through rules or minimal generalisations. Participants see different verbs in the ESP task and the posttest. The rules that apply to these verbs will overlap. This will be especially true for rules that have broad structural descriptions and thus apply to many forms. These tend to be regular rules.

```{r ruleoverlap}

rule_table = d |> 
  filter(
    category == 'sang',
    best_rule_word_type,
    phase != 'pretest',
    !is.na(rule)
         ) |> 
  filter(participant_id == 's6023') |> 
  distinct(phase,rule_tidy,rule_id,type,scope) |> 
  group_by(rule_tidy,rule_id,type,scope) |> 
  arrange(phase) |> 
  summarise(phase = paste(phase, collapse = ', ')) |>
  mutate(phase = factor(phase, levels = c('esp, posttest', 'esp', 'posttest'))) |> 
  ungroup() |> 
  select(-rule_id) |> 
  arrange(phase,-scope) |> 
  mutate(
    rule_tidy = str_replace_all(rule_tidy, c('1 ' = '+', '0 ' = '-'))
  )

r1 = nrow(rule_table[rule_table$phase =='esp, posttest',])
r2 = nrow(rule_table[rule_table$phase =='esp',])
r3 = nrow(rule_table[rule_table$phase =='posttest',])
r4 = round(mean(rule_table[rule_table$phase =='esp, posttest',]$scope),0)
r5 = round(mean(rule_table[rule_table$phase !='esp, posttest',]$scope),0)
r6 = nrow(rule_table[rule_table$phase == 'esp, posttest' & rule_table$type == 'regular',])
r7 = nrow(rule_table[rule_table$phase != 'esp, posttest' & rule_table$type == 'regular',])

rule_table |>
  write_sheet('https://docs.google.com/spreadsheets/d/1RE2MWEZhyh4cwsOHek_T8VkAQ3O5aiowtTeE_meL1lU/edit?usp=sharing', 'table6')

rule_table |>
  kable('simple', caption = 'Table 6. Rule overlaps for one participant (sang verbs)', digits = 2)

```

To illustrate this point, we show one participant and one category, 'sang', in Table 6. Taken together, `r r1+r2+r3` relevant rules apply to the 104 verbs that our participant sees in the ESP task and the posttest. `r r1` rules overlap: they apply to some verbs in both tasks. `r r2` rules only apply to verbs in the ESP task. Following the MGL logic, whatever the participant learned about these verbs won't carry over to the posttest. `r r3` rules only apply to verbs in the posttest. The participant didn't learn anything new about these verbs. Unsurprisingly, the rules that overlap have much larger scopes (a mean of `r r4`) than those which do not (a mean of `r r5`). `r r6` overlapping rules are regular, only `r r7` non-overlapping rule is regular.

This suggests that whatever the participant learns about the regular rules will be far more influential in the posttest than what they learn about irregular rules.

One way to express learning is to adjust rule confidence for rules in the ESP task. We will do this in the following way: For each participant and rule in the ESP task, we tally the number of regular and irregular responses by the participant in the ESP task. We do this for the reversed condition only.

If the rule is regular, it works very well if every verb it applies gets a regular response. For every regular response, the rule is rewarded. For every irregular response, the rule incurs a penalty. If the rule is irregular, all the verbs in its scope should be irregular. For every irregular response, the rule is rewarded. For every regular response, the rule incurs a penalty.

These total rewards and penalties are used to update the rule's confidence. If the participant starts the ESP task with a very strong regular rule but keeps picking irregular forms for verbs in the rule's scope, the rule's confidence is steadily demoted. If the rule works well across the task, its confidence will increase. The rate of this increase / decrease is controlled by the parameter `learning rate`, which ranges between 0.5 and 25, with a .5 step. We fit the rule updater with two extra hyperparameters. These are (a) rule type ((i) updating both regular and irregular rules, (ii) updating regular rules only, (iii) updating irregular rules only) and (b) response type ((i) updating based on co-player responses or (ii) the participant's own responses in the ESP test phase).

```{r rulemove1, fig.width = 9, fig.height = 4}
curves |> 
  mutate(which_responses2 = glue('{which_responses} responses')) |> 
  ggplot(aes(learning_rate,c, colour = which_rules)) +
  geom_line() +
  facet_wrap(~ which_responses2) +
  scale_colour_colorblind() +
  theme_bw() +
  labs(colour = 'rules updated') +
  xlab('learning rate') +
  ylab("Somers C\n(Dxy rank correlation receiver\noperating characteristic curve area)")
```

Figure 5 shows how the updated rules fare in the reversed condition posttest. The x axis shows model learning rate, the y axis shows model accuracy, as expressed by the concordance index 'C' of the individual participant responses and the model prediction. The colours show which rules were updated in a given model. The two panels show models trained on player or co-player responses. The best model is trained on the participant's own responses, tunes regular rules only, and has a comparatively high learning rate. This can be seen in Table 7.

```{r best_outcomes}
best_outcomes = outcomes |> 
  distinct(learning_rate,which_rules,which_responses,c) |> 
  arrange(-c)

best_outcomes |> 
  write_sheet('https://docs.google.com/spreadsheets/d/1RE2MWEZhyh4cwsOHek_T8VkAQ3O5aiowtTeE_meL1lU/edit?usp=sharing', 'table8')
best_outcomes |> 
  kable(digits = 2, caption = 'Table 7. Best outcomes for the updating model')

best_settings = best_outcomes |> 
  select(-c) |> 
  slice(1)
```

How does this compare with the original models?

We fit a hierarchical logistic regression model predicting participant responses across the reversed condition, using baseline and individual model weights as a predictor and a participant grouping factor. We compare the GCM, the baseline + individual MGL, and the baseline + updating MGL. We fit separate models because of collinearity issues.

```{r comparison_reversed}

# sanity check
# with(test2_2, Hmisc::somers2(baseline_gcm_features,resp_post_reg))
# with(test2_2, Hmisc::somers2(baseline_mgl_features,resp_post_reg))
# with(test2_2, Hmisc::somers2(individual_gcm_features,resp_post_reg))
# with(test2_2, Hmisc::somers2(individual_mgl_features,resp_post_reg))
# with(test2_2, Hmisc::somers2(baseline_mgl_features_updating,resp_post_reg))
# I'm sane

fit_gcm = glmer(resp_post_reg ~ baseline_gcm_features + individual_gcm_features + (1|participant_id), data = test2_2, family = binomial)
tidy(fit_gcm, conf.int = T)
# check_collinearity(fit_gcm)
fit_mgl_1 = glmer(resp_post_reg ~ baseline_mgl_features + individual_mgl_features + (1|participant_id), data = test2_2, family = binomial)
tidy(fit_mgl_1, conf.int = T)
# check_collinearity(fit_mgl_1)
fit_mgl_2 = glmer(resp_post_reg ~ baseline_mgl_features + baseline_mgl_features_updating + (1|participant_id), data = test2_2, family = binomial)
# tidy(fit_mgl_2, conf.int = T)

compo_table = compare_performance(fit_gcm,fit_mgl_1,fit_mgl_2, metrics = 'common')

compo_table |> 
   write_sheet('https://docs.google.com/spreadsheets/d/1RE2MWEZhyh4cwsOHek_T8VkAQ3O5aiowtTeE_meL1lU/edit?usp=sharing', 'table9')

compo_table

```

In this set, the baseline + updating MGL is the best fit. There is more mileage in tuning the GCM and the original MGL as well, but finding an MGL setup that improves on the baseline is an important result.

If we ignore the hierarchy of the data, we can plot ROC curves for each model.

```{r roc_curve, fig.width=5,fig.height=3}

roc_data = test2_2 |> 
  distinct(resp_post_reg,individual_gcm_features,individual_mgl_features,baseline_mgl_features_updating) |> 
  pivot_longer(cols = -resp_post_reg, names_to = "model", values_to = "prediction") |> 
  group_by(model)  |> 
  summarise(roc_obj = list(roc(resp_post_reg, prediction)))


# Create a data frame with the ROC curve data
roc_df = roc_data %>%
  mutate(
    tpr = map(roc_obj, ~ .$sensitivities),
    fpr = map(roc_obj, ~ 1 - .$specificities),
    auc = map_dbl(roc_obj, ~ .$auc)
  ) %>%
  unnest(c(tpr, fpr))

roc_df |> 
  mutate(
    model2 = case_when(
      str_detect(model, 'updating') ~ 'rule updating model',
      str_detect(model, 'individual_mgl') ~ 'rule building model',
      str_detect(model, 'gcm') ~ 'analogical model'
    ),
    model2 = glue('{model2}, AUC: {round(auc,2)}')
  ) |> 
  ggplot(aes(x = fpr, y = tpr, color = model2)) +
  geom_line() +
  geom_abline(linetype = "dashed", color = "grey") +
  labs(
    title = "ROC Curves",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Model"
  ) +
  theme_minimal() +
  scale_colour_colorblind()
```

## Discussion

We trained the Minimal Generalisation Learner on real English verbs and tested it on results of a forced-choice Wug task in which participants had to pick the regular or irregular past tense form for nonwords. Like in earlier work (Albright & Hayes 2003), the MGL can predict participant behaviour in a Wug task.

We also tested the MGL on results from a morphological convergence experiment in which participants are exposed to coplayers and have to agree with them on choices in a forced-choice Wug task. Participants will converge to the lexical distributions of the coplayer and subsist with this distribution in a subsequent posttest, extending it to novel noword forms.

Since the MGL was trained on real verbs which comprise a typical lexical distribution, its predictions will be less accurate when measured against participant choices after participants have been exposed to a coplayer who reverses the typical distribution. We can tune the MGL to better fit participant responses by using data from the participant's interaction with the coplayer and updating each MGL rule based on participant choices during this interaction.

```{r regulars_lol}
percent_irreg = d |> 
  distinct(word,baseline_log_odds_regular) |> 
  summarise(mean = mean(-baseline_log_odds_regular)) |> 
  pull() |> 
  plogis() |> 
  round(2)

percent_irreg = percent_irreg * 100

```

The morphological convergence experiment is unusual in how much it showcases English irregular inflectional morphology. In reality, the vast majority of existing English verb types is regular and irregular lexical gangs take on new members very infrequently. In the experiment, the use of irregular forms is rampant, both by the coplayers and the participants (the mean rate of use for regular forms was `r percent_irreg`% in the baseline task).

One would then assume that if participants operate by establishing and updating minimal morphophonological generalisations, they would tackle the reversed coplayer by updating the irregular generalisations, establishing heuristics such as "if words look like <<sing>> they are much more likely to have a past tense like <<sang>> today". However, our simulations show that the best way to capture the change in participant behaviour is to update the regular rules only. This is likely because these rules apply to many forms: A regular rule adjusted by many verbs in the ESP task will apply to many verbs in the posttest. In contrast, while updating irregular rules may well better capture the shifts we see in participant behaviour, they are too fragmented for this to carry over from the learning phase (the ESP task) to retesting (the posttest). This would mean that the heuristic above should be reformulated as: "all words that look really regular should actually be less regular today".

